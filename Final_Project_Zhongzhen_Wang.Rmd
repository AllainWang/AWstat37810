---
title: "Final Project (only by Zhongzhen Wang)"
author: "Zhongzhen Wang"
date: "2015Äê11ÔÂ5ÈÕ"
output: html_document
---

$$\textbf{Question 1 Metropolis-Hastings}$$
The algorithm is as the follow:
```{r}
mh.beta <- function(iterations, startvalue, burnin, shape1, shape2, c){
  theta.cur <- startvalue
#choose theta as the start value
  draws <- c()
  theta.update <- function(theta.cur, shape1, shape2){
    theta.can <- rbeta(1, shape1, shape2)
#the new theta candidate is generated from a beta distribution
    accept.prob <- rbeta(1, c*theta.can, c*(1-theta.can))
#I set the acceptance probability as the distribution required by the question 1
    if(runif(1) <= accept.prob)
      theta.can
    else
      theta.cur
  }
#If the acceptance probability is higher than a number generated by the uniform distribution from 0 to 1, we accept the new theta candidate, otherwise, the old theta, then I construct a vector of samples
  for(i in 1:iterations){
    draws[i] <- theta.cur <- theta.update(theta.cur, shape1 = shape1, shape2 = shape2)
  }
  return(draws[(burnin+1):iterations])
}
```


Now, I plot a trace plot of this sampler and an autocorrelation plot, as well as a histogram of beta(10000, 6,4)
```{r}
par(mfrow=c(1,3))
plot(rbeta(10000, 6, 4)); acf(rbeta(10000, 6, 4)); hist(rbeta(10000, 6, 4))
```

Then, I plot a trace plot of this sampler and an autocorrelation plot, as well as a histogram of the draws by setting different c's
```{r}
mh.draws <- mh.beta(10000, startvalue = runif(1), burnin = 1000, shape1 = 6, shape2 = 4, c=1)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(mh.draws); acf(mh.draws); hist(mh.draws)  #plot commands

mh.draws <- mh.beta(10000, startvalue = runif(1), burnin = 1000, shape1 = 6, shape2 = 4, c=0.1)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(mh.draws); acf(mh.draws); hist(mh.draws)  #plot commands

mh.draws <- mh.beta(10000, startvalue = runif(1), burnin = 1000, shape1 = 6, shape2 = 4, c=2.5)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(mh.draws); acf(mh.draws); hist(mh.draws)  #plot commands
 
mh.draws <- mh.beta(10000, startvalue = runif(1), burnin = 1000, shape1 = 6, shape2 = 4, c=10)
par(mfrow=c(1,3))  #1 row, 3 columns
 plot(mh.draws); acf(mh.draws); hist(mh.draws)  #plot commands
```
From the comparison of these draws, c = 2.5 is probably the best model within the above models.



$$\textbf{Question 2 Gibbs sampling}$$
Firstly, I set $$h = h(x) = ye^{-yx}$$. So the inverse function is $$x = h^{-1}(x) = -\frac{1}{y}\log{(\frac{h}{y})}$$.
And similiarly, $$g = g(y) = xe^{-yx}$$. So the inverse function is $$y = g^{-1}(y) = -\frac{1}{x}\log{(\frac{g}{x})}$$.
The code for the inverse function is shown below, with setting $$0 < x, y < B = 5$$
```{r}
 x <- 1
 y <- 1
invcdfh <- function(h,y,B=5) {
  while( x < B){
  return((-1/y)*log(h/y))
}
}

invcdfg <- function(g,x,B=5) {
  while( y < B){
  return((-1/x)*log(g/x))
}
}
```


The algorithm of the Gibbs Sampling is as the follow,
```{r}
gibbs<-function (n) 
{
  sampleh <- sapply(runif(n), invcdfh, y)
  sampleg <- sapply(runif(n), invcdfg, x)
  mat <- matrix(ncol = 2, nrow = n)
 
  mat[1, ] <- c(x, y)
  for (i in 2:n) {
    x <- sampleh[i]
    y <- sampleg[i]
    mat[i, ] <- c(x, y)
  }
  mat
}

```

The histgrams of Gibbs with different sample sizes, T = 500, 5000 and 50000
```{r}
par(mfrow=c(1,3))
hist(gibbs(500))  #plot commands
hist(gibbs(5000))  #plot commands 
hist(gibbs(50000))  #plot commands 
```


Now, we need to consider the moments of marginal distribution. We have
$$\mu_{x} = \frac{1}{M}\sum_{i=1}^M x_i^{N} P(x = x_{i})$$
```{r}
#N is the N-th degree, T is the iteration
moment <- function(N,T){
  sampleh <- sapply(runif(T), invcdfh, y)
  sig <- (sampleh[1])^N*y*exp(-y*sampleh[1])
  for(j in 2:T){
    mome <- (sampleh[j])^N*y*exp(-y*sampleh[j])
  sig <- sig + mome
  }
  sig/T
}
```

For the expectation of x, we set N = 1, T = 500, 5000, 50000
```{r} 
expect1 <- moment(1,500) 
expect2 <- moment(1,5000) 
expect3 <- moment(1,50000)
c(expect1, expect2, expect3)
```


$$\textbf{Question 3 K-Means}$$
Firstly, we try the Wine model without scaling
```{r}
install.packages('rattle',repos = "http://cran.us.r-project.org")
data(wine, package="rattle")

data.train.un <- wine[-1]
#Since the first column in the dataframe is the type of wine(1,2,3), so we need to rule it out of the model. 

install.packages('NbClust',repos = "http://cran.us.r-project.org")
library(NbClust)
nc <- NbClust(data.train.un, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]), xlab="Numer of Clusters", ylab="Number of Criteria")
#According to the method, we can find the best number of clusters is 2. 
#We now fit wine data to K-Means with k = 2

fit.km.un <- kmeans(data.train.un, 2)

library(fpc)
plotcluster(data.train.un, fit.km.un$cluster)
#We can see the data is not clustered very well.

library(MASS)
parcoord(data.train.un, fit.km.un$cluster)

confuseTable.km.un <- table(wine$Type, fit.km.un$cluster)
confuseTable.km.un

install.packages('flexclust',repos = "http://cran.us.r-project.org")
library(flexclust)
randIndex(confuseTable.km.un)
#It is not close to 1 so K-Means is not good model for clustering wine data set without scaling.
```

Then, we try the Wine model with scaling
```{r}
#Set a new dataframe to scale the existing data Wine
data.train <- scale(wine[-1])

nc <- NbClust(data.train.un, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]), xlab="Numer of Clusters", ylab="Number of Criteria")
#According to the method, we can find the best number of clusters is 3. 
#We now fit wine data to K-Means with k = 3

fit.km <- kmeans(data.train, 3)

library(fpc)
plotcluster(data.train, fit.km$cluster)
#We can see the data is clustered very well.

library(MASS)
parcoord(data.train, fit.km$cluster)

confuseTable.km <- table(wine$Type, fit.km$cluster)
confuseTable.km

library(flexclust)
randIndex(confuseTable.km)
#It is close to 1 so K-Means is good model for clustering wine data set with scaling.
```

We try the iris model without scaling
```{r}
#At the beginning, we need to label "setosa", "versicolor" and "virginica" as 1, 2 and 3
data <- iris
as.integer(data$Species)
x=as.integer(data$Species)
data[,5] <- x

nc <- NbClust(data[-5], min.nc=2, max.nc=10, method="kmeans")
barplot(table(nc$Best.n[1,]), xlab="Numer of Clusters", ylab="Number of Criteria")
#According to the method, we can find the best number of clusters is 2. 
#We now fit wine data to K-Means with k = 2.


fit.km.iris <- kmeans(data[-5], 2)

library(fpc)
plotcluster(data[-5], fit.km.iris$cluster)
#We can see the data is clustered very well except a couple of points

parcoord(data[-5], fit.km.iris$cluster)

confuseTable.km.iris <- table(data$Species, fit.km.iris$cluster)

library(flexclust)
randIndex(confuseTable.km.iris)
#It is quite close to 1 so K-Means is a relatively good model for clustering wine data set without scaling.
```

Now, we try the iris model with scaling
```{r}
#Set a new dataframe to scale the existing data iris
data.re <- scale(data[-5])

nc <- NbClust(data.re, min.nc=2, max.nc=10, method="kmeans")
barplot(table(nc$Best.n[1,]), xlab="Numer of Clusters", ylab="Number of Criteria")
#According to the method, we can find the best number of clusters is 3. 
#We now fit wine data to K-Means with k = 3


fit.km.iris <- kmeans(data.re, 3)

library(fpc)
plotcluster(data.re, fit.km.iris$cluster)
#We can see the data is not clustered very well, especially in the union of 1's and 2's

parcoord(data.re, fit.km.iris$cluster)

confuseTable.km.iris <- table(data$Species, fit.km.iris$cluster)

library(flexclust)
randIndex(confuseTable.km.iris)
#It is not close to 1 so K-Means is not a good model for clustering wine data set with scaling.
```


